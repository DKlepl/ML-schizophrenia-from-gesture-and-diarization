---
title: "Data preprocessing"
output: html_document
---

```{r setup, include=FALSE}
pacman::p_load(tidyverse, hms, tuneR)
```

# 1) Data overview
  A) 'Audio' contains recordings from the interviews in wav format.
  B) 'Diarization' contains turn-taking data automatically extracted from the audio recordings. There are raw data in .seg format and cleaned up data in .csv.
  C) 'Gesture' containts raw data from the actigraphs (wrist movement sensors). This was originally measured as changes in acceleration on 3 axis which were meaningless and so euclidian distance was calculated.
  D) 'Timing_Handedness.txt' contains hand-notes about start time of the gesturing and voice start, handedness of the subject and schizophrenia symptoms.

## 1. Build/find an algorithm for claps detection in audio
```{r audio algorithm build-up}
#read audio file
audio_path = "raw_data/Audio"
audio_files = list.files(path=audio_path, full.names = T) 
f = audio_files[1]

#subject ID
filename = strsplit(f, '/')[[1]][3]
id = strsplit(filename, '[.]')[[1]][1]

#save also diagnosis
if (startsWith(id,"1")==T) {
  diagnosis=1
} else {
  diagnosis=0
  }

audio = readWave(f)
sampling = audio@samp.rate #22050

test = audio@left[(0):(10*sampling)]

#use soundgen's algorithm to find bursts of acoustic energy in the signal
#it's intended for syllable segmentation but it works well
library(soundgen)
seg = segment(test, samplingRate = sampling,windowLength = 40, plot = T )
bursts = seg$bursts

#how to identify which burst are claps?
#calculate SD with a sliding window - the lowest = 2 bursts small interburst interval
library(evobiR)
sd= SlidingWindow("sd",bursts$interburstInt , 2, 1)
sd = c(sd,rep(NA,2))
bursts$sd = sd

#get the time of the first clap and show plot
clap = which.min(bursts$sd)
seg = segment(test, samplingRate = sampling,windowLength = 40, plot = T )
check = as.numeric(readline("Which burst is the first clap? "))

if (clap != check) {
  clap = check
}

#get the time of the claps
three_claps = data.frame(audioclap1 = bursts[clap, 1], audioclap2 = bursts[clap+1, 1], audioclap3 = bursts[clap+2, 1])
three_claps = three_claps/1000
```

This algorithm works well enough (0.8 accuracy). So now I'll wrap the code from the chunk above into a nice small functions. Also I'll build a sanity check into the function so that I can prevent any mistakes even before appearing in the final output. All functions are in R script "preprocessing_functions.R"

Now loop through all audio files in the folder and gather the claps into one dataframe
```{r}
source("preprocessing_functions.R")

audio_path = "raw_data/Audio"
audio_files = load_folder(audio_path)
#place to store all claps
audio_claps = data.frame()

for (file in audio_files) {
  one_output = try(get_claps(file))
  
  #rbind the output to the big dataframe
  audio_claps = try(rbind(audio_claps, one_output))
}

#save the output
write.csv(audio_claps, "clean_data/audio_claps.csv")
```


Find claps in the actigraphs
```{r}
#gesture sample
gest = read.csv('raw_data/Gesture/Sub103RightHanded.csv')
gest = na.omit(gest)

#sampling is 100 Hz => 1 datapoint = 10 ms
sampling_acti = 100
test = gest

#this is how the claps look like in the actigraph
ggplot(gest[48000:49000,])+
   geom_line(aes((48000:49000)/100,PsychologistJerkLeft),color="blue")

ggplot(gest[48000:49000,])+
   geom_line(aes((48000:49000)/100,PsychologistJerkRight),color="blue")

#find peaks in each hand
#subtract the peaks - the 3 closest to each other in time are the claps
library(pracma)
plot(test$PsychologistJerkLeft, type="l", col="navy")
x_left = findpeaks(test$PsychologistJerkLeft, minpeakheight = 2, npeaks = 10)
points(x_left[, 2], x_left[, 1],col="red", pch=20)

plot(test$PsychologistJerkRight, type="l", col="navy")
x_right = findpeaks(test$PsychologistJerkRight, minpeakheight = 2, npeaks = 10)
points(x_right[, 2], x_right[, 1],col="red", pch=20)


x = cbind(x_left,x_right)
x = as.data.frame(x)
x$diff = x_left[,2] - x_right[,2]
```

```{r clean Timing_Handedness.txt}
#strinAsFactors=F because time-data would get converted to factors
Timing = read.delim("raw_data/Timing_Handedness.txt", na.strings="NaN", stringsAsFactors=F)

#all time-data needs to be converted to hms format from strings - I'll need to calculate with them
#Diff is difference between VoiceStart and GestureStart in seconds
Timing = Timing %>% mutate(
  GestureStart=as.hms(GestureStart),
  VoiceStart=as.hms(VoiceStart),
  Diff=VoiceStart-GestureStart
)
```
  
  

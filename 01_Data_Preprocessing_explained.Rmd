---
title: "Data preprocessing"
output: html_document
---

```{r setup, include=FALSE}
pacman::p_load(tidyverse, hms, tuneR)
```

# 1) Data overview
  A) 'Audio' contains recordings from the interviews in wav format.
  B) 'Diarization' contains turn-taking data automatically extracted from the audio recordings. There are raw data in .seg format and cleaned up data in .csv.
  C) 'Gesture' containts raw data from the actigraphs (wrist movement sensors). This was originally measured as changes in acceleration on 3 axis which were meaningless and so euclidian distance was calculated.
  D) 'Timing_Handedness.txt' contains hand-notes about start time of the gesturing and voice start, handedness of the subject and schizophrenia symptoms.

## 1. Build/find an algorithm for claps detection in audio
```{r audio algorithm build-up}
#read audio file
audio_path = "raw_data/Audio"
audio_files = list.files(path=audio_path, full.names = T) 
f = audio_files[74]

#subject ID
filename = strsplit(f, '/')[[1]][3]
ID = strsplit(filename, '[.]')[[1]][1]


#save also diagnosis
if (startsWith(ID,'1')==T) {
  diagnosis=1
} else {
  diagnosis=0
}

audio = readWave(f)
sampling = audio@samp.rate #22050

test =as.numeric(audio@left[(0*sampling):(4*sampling)])
rm(audio)

#use soundgen's algorithm to find bursts of acoustic energy in the signal
#it's intended for syllable segmentation but it works well
library(soundgen)
seg = segment(test, samplingRate = sampling, plot = T )
bursts = seg$bursts

#how to identify which burst are claps?
#calculate SD with a sliding window - the lowest = 2 bursts small interburst interval
library(evobiR)
sd= SlidingWindow("sd",bursts$time , 3, 1)


#get the time of the first clap and show plot
clap = which.min(sd)
seg = segment(test, samplingRate = sampling,windowLength = 40, plot = T )
check = as.numeric(readline("Which burst is the first clap? "))

if (clap != check) {
  clap = check
}

#get the time of the claps
three_claps = data.frame(audioclap1 = bursts[clap, 1], audioclap2 = bursts[clap+1, 1], audioclap3 = bursts[clap+2, 1])
three_claps = three_claps/1000
three_claps = three_claps+342
```

This algorithm works well enough (0.8 accuracy). So now I'll wrap the code from the chunk above into a nice small functions. Also I'll build a sanity check into the function so that I can prevent any mistakes even before appearing in the final output. All functions are in R script "preprocessing_functions.R"

Now loop through all audio files in the folder and gather the claps into one dataframe
```{r}
audio_path = "raw_data/Audio"
audio_files = load_folder(audio_path)
#place to store all claps
audio_claps = data.frame()

for (file in audio_files) {
  one_output = try(get_claps(file))
  
  #rbind the output to the big dataframe
  audio_claps = try(rbind(audio_claps, one_output))
}

#save the output
write.csv(audio_claps, "clean_data/audio_claps.csv")
```


## 1. Build/find an algorithm for claps detection in actigraph data
Similarly to finding the 3 claps in the audio recordings we'll be looking for 3 relatively rhytmic peaks in the signal.
We can exploit that we have 2 signals here; from each hand. Therefore the 3 claps can be quite simply found by finding peaks in both signals and then subtracting these peaks. Peaks very close to each other in both signals are most probably the claps. It's unlikely that both hands would move in such way to create very similar pattern in the signal.
```{r}
#import preprocessing functions from the script
source("Scripts/preprocessing_functions.R")

#load files from the whole folder
path_gesture = "raw_data/Gesture"
gesture_files = load_folder(path = path_gesture)

#sampling is 100 Hz => 1 datapoint = 10 ms
sampling_gesture = 100

#get one file as a testing for all the processing
g = gesture_files[11]

#get info about the participant out of the name of the file - only difference from audio file is that gesture includes also hand dominance - so the get_info function is updated with this bit
if (grepl("Right", g) == TRUE) {
  right = 1
} else {
  right = 0
}

info = 

#open the file
gest = read.csv(g)

#because the gestures are measured as change of accelaration on 3-axis the first row contains always NAs => remove those
gest = na.omit(gest)

#create a time variable - in seconds - mostly for easier orientation
gest$time = 1:nrow(gest)/100

#this is how the claps look like in the actigraph (use subject 101)
ggplot(gest[48000:49000,])+
   geom_line(aes((48000:49000)/100,PsychologistJerkLeft),color="blue")

#find peaks in both hands
#subtract the peaks (all combinations) - the 3 closest to each other in time are the claps
library(pracma)
#find peaks with regex (neighboring points are lower)
plot(gest$PsychologistJerkLeft, type="l", col="navy")
x_left = as.data.frame(findpeaks(gest$PsychologistJerkLeft, minpeakheight = 2, npeaks = 100, minpeakdistance = 20))
points(x_left[, 2], x_left[, 1],col="red", pch=20)

plot(gest$PsychologistJerkRight, type="l", col="navy")
x_right = as.data.frame(findpeaks(gest$PsychologistJerkRight, minpeakheight = 2, npeaks = 100, minpeakdistance = 20))
points(x_right[, 2], x_right[, 1],col="red", pch=20)

#permutate x_right and x_left
peak_permutation = expand.grid(x_left$V2,x_right$V2)

#subtract peaks from each other and take absolute value
peak_permutation$diff = abs(peak_permutation$Var1 - peak_permutation$Var2)

#remove peaks that are more than 1 second away from each other
peak_permutation = peak_permutation[peak_permutation$diff<100,]
peak_permutation = peak_permutation[order(peak_permutation$Var1),]

#get the remaining peaks
unique_peaks = unique(peak_permutation$Var1)


#in 99% cases claps are the first 3 peaks
#the claps should have sd no more than 100 (1 second)

#just check that the peaks are not very far away from each other
if (length(unique_peaks)>3) {
  sd= evobiR::SlidingWindow("sd", unique_peaks , 3, 1)
  continue = sd[1]<100
} else {
  continue = sd(unique_peaks)<100
}

if (continue==T) {
  gesture_claps = data.frame(gestclap1 = unique_peaks[1], 
                             gestclap2 = unique_peaks[2], 
                             gestclap3 = unique_peaks[3])
  gesture_claps = gesture_claps/100
} else {
  gesture_claps = data.frame(gestclap1 = NA, 
                             gestclap2 = NA, 
                             gestclap3 = NA)
}
```

Now we can compare the difference of the extracted claps with the gap between manual notes about the start times.
```{r Timing_Handedness.txt}
#strinAsFactors=F because time-data would get converted to factors
Timing = read.delim("raw_data/Timing_Handedness.txt", na.strings="NaN", stringsAsFactors=F)

#all time-data needs to be converted to hms format from strings - I'll need to calculate with them
#Diff is difference between VoiceStart and GestureStart in seconds
Timing = Timing %>% mutate(
  GestureStart=as.hms(GestureStart),
  VoiceStart=as.hms(VoiceStart),
  Diff=VoiceStart-GestureStart
)
```
  
  

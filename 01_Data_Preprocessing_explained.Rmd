---
title: "Data preprocessing"
output: html_document
---

```{r setup, include=FALSE}
pacman::p_load(tidyverse, hms, tuneR)
```

# 1) Data overview
  A) 'Audio' contains recordings from the interviews in wav format.
  B) 'Diarization' contains turn-taking data automatically extracted from the audio recordings. There are raw data in .seg format and cleaned up data in .csv.
  C) 'Gesture' containts raw data from the actigraphs (wrist movement sensors). This was originally measured as changes in acceleration on 3 axis which were meaningless and so euclidian distance was calculated.
  D) 'Timing_Handedness.txt' contains hand-notes about start time of the gesturing and voice start, handedness of the subject and schizophrenia symptoms.

## 1. Build/find an algorithm for claps detection in audio
```{r audio algorithm build-up}
#read audio file
audio_path = "raw_data/Audio"
audio_files = list.files(path=audio_path, full.names = T) 
f = audio_files[74]

#subject ID
filename = strsplit(f, '/')[[1]][3]
ID = strsplit(filename, '[.]')[[1]][1]


#save also diagnosis
if (startsWith(ID,'1')==T) {
  diagnosis=1
} else {
  diagnosis=0
}

audio = readWave(f)
sampling = audio@samp.rate #22050

test =as.numeric(audio@left[(0*sampling):(4*sampling)])
rm(audio)

#use soundgen's algorithm to find bursts of acoustic energy in the signal
#it's intended for syllable segmentation but it works well
library(soundgen)
seg = segment(test, samplingRate = sampling, plot = T )
bursts = seg$bursts

#how to identify which burst are claps?
#calculate SD with a sliding window - the lowest = 2 bursts small interburst interval
library(evobiR)
sd= SlidingWindow("sd",bursts$time , 3, 1)


#get the time of the first clap and show plot
clap = which.min(sd)
seg = segment(test, samplingRate = sampling,windowLength = 40, plot = T )
check = as.numeric(readline("Which burst is the first clap? "))

if (clap != check) {
  clap = check
}

#get the time of the claps
three_claps = data.frame(audioclap1 = bursts[clap, 1], audioclap2 = bursts[clap+1, 1], audioclap3 = bursts[clap+2, 1])
three_claps = three_claps/1000
three_claps = three_claps+342
```

This algorithm works well enough (0.8 accuracy). So now I'll wrap the code from the chunk above into a nice small functions. Also I'll build a sanity check into the function so that I can prevent any mistakes even before appearing in the final output. All functions are in R script "preprocessing_functions.R"

Now loop through all audio files in the folder and gather the claps into one dataframe
```{r}
audio_path = "raw_data/Audio"
audio_files = load_folder(audio_path)
#place to store all claps
audio_claps = data.frame()

for (file in audio_files) {
  one_output = try(get_claps(file))
  
  #rbind the output to the big dataframe
  audio_claps = try(rbind(audio_claps, one_output))
}

#save the output
write.csv(audio_claps, "clean_data/audio_claps.csv")
```


## 1. Build/find an algorithm for claps detection in actigraph data
Similarly to finding the 3 claps in the audio recordings we'll be looking for 3 relatively rhytmic peaks in the signal.
We can exploit that we have 2 signals here; from each hand. Therefore the 3 claps can be quite simply found by finding peaks in both signals and then subtracting these peaks. Peaks very close to each other in both signals are most probably the claps. It's unlikely that both hands would move in such way to create very similar pattern in the signal.
```{r}
#import preprocessing functions from the script
source("Scripts/preprocessing_functions.R")

#load files from the whole folder
path_gesture = "raw_data/Gesture"
gesture_files = load_folder(path = path_gesture)

#sampling is 100 Hz => 1 datapoint = 10 ms
sampling_gesture = 100

#get one file as a testing for all the processing
g = gesture_files[11]

#get info about the participant out of the name of the file - only difference from audio file is that gesture includes also hand dominance - so the get_info function is updated with this bit
if (grepl("Right", g) == TRUE) {
  right = 1
} else {
  right = 0
}

info = 

#open the file
gest = read.csv(g)

#because the gestures are measured as change of accelaration on 3-axis the first row contains always NAs => remove those
gest = na.omit(gest)

#create a time variable - in seconds - mostly for easier orientation
gest$time = 1:nrow(gest)/100

#this is how the claps look like in the actigraph (use subject 101)
ggplot(gest[48000:49000,])+
   geom_line(aes((48000:49000)/100,PsychologistJerkLeft),color="blue")

#find peaks in both hands
#subtract the peaks (all combinations) - the 3 closest to each other in time are the claps
library(pracma)
#find peaks with regex (neighboring points are lower)
plot(gest$PsychologistJerkLeft, type="l", col="navy")
x_left = as.data.frame(findpeaks(gest$PsychologistJerkLeft, minpeakheight = 2, npeaks = 100, minpeakdistance = 20))
points(x_left[, 2], x_left[, 1],col="red", pch=20)

plot(gest$PsychologistJerkRight, type="l", col="navy")
x_right = as.data.frame(findpeaks(gest$PsychologistJerkRight, minpeakheight = 2, npeaks = 100, minpeakdistance = 20))
points(x_right[, 2], x_right[, 1],col="red", pch=20)

#permutate x_right and x_left
peak_permutation = expand.grid(x_left$V2,x_right$V2)

#subtract peaks from each other and take absolute value
peak_permutation$diff = abs(peak_permutation$Var1 - peak_permutation$Var2)

#remove peaks that are more than 1 second away from each other
peak_permutation = peak_permutation[peak_permutation$diff<100,]
peak_permutation = peak_permutation[order(peak_permutation$Var1),]

#get the remaining peaks
unique_peaks = unique(peak_permutation$Var1)


#in 99% cases claps are the first 3 peaks
#the claps should have sd no more than 100 (1 second)

#just check that the peaks are not very far away from each other
if (length(unique_peaks)>3) {
  sd= evobiR::SlidingWindow("sd", unique_peaks , 3, 1)
  continue = sd[1]<100
} else {
  continue = sd(unique_peaks)<100
}

if (continue==T) {
  gesture_claps = data.frame(gestclap1 = unique_peaks[1], 
                             gestclap2 = unique_peaks[2], 
                             gestclap3 = unique_peaks[3])
  gesture_claps = gesture_claps/100
} else {
  gesture_claps = data.frame(gestclap1 = NA, 
                             gestclap2 = NA, 
                             gestclap3 = NA)
}
```

### CHECK
Now we compare the difference of the extracted claps with the gap between manual notes about the start times.
```{r Timing_Handedness.txt}
#strinAsFactors=F because time-data would get converted to factors
Timing = read.delim("raw_data/Timing_Handedness.txt", na.strings="NaN", stringsAsFactors=F)

#rename columns to the same names I used for audio and gesture claps files
colnames(Timing)[1]="ID"

#column 'Handedness in Timing denotes right dominance with 0 while my files use 1
#switch the values
Timing[Timing$Handedness==0, 'right']=1
Timing[Timing$Handedness==1, 'right']=0

#remove Handedness
Timing = Timing[,c(-3,-7)]

#all time-data needs to be converted to hms format from strings - I'll need to calculate with them
#Diff is difference between VoiceStart and GestureStart in seconds

library(hms)
Timing = Timing %>% mutate(
  GestureStart=as.hms(GestureStart),
  VoiceStart=as.hms(VoiceStart),
  Diff=VoiceStart-GestureStart
)

#load audio and gesture claps
a_claps = read.csv("clean_data/audio_claps.csv")
g_claps = read.csv("clean_data/gesture_claps.csv")

claps = merge(a_claps, g_claps, by=c("ID","diagnosis"))

#calculate the gap between all three claps, than use average gap to compare with the hand-notes - round to 2 decimals because that's sampling of actigraphs
claps$diff_1=round(claps$audioclap1-claps$gestclap1,2)
claps$diff_2=round(claps$audioclap2-claps$gestclap2,2)
claps$diff_3=round(claps$audioclap3-claps$gestclap3,2)

claps$Gap = apply(claps[,10:12],1,mean)
claps$diff_sd = apply(claps[,10:12],1,sd)

write.csv(claps , "clean_data/all_claps.csv", row.names = F)

all = merge(Timing ,claps[,c(1,13)], by='ID')
all$check_diff = abs(all$Diff-all$Gap)

#look at difference between manual notes and automated gap calculation
mean(all$check_diff, na.rm = T)
sd(all$check_diff, na.rm = T)
range(all$check_diff, na.rm = T)
```

# Merging data
Now we can merge gesture files with diarization files. In other words tagging all datapoints from actigraph by the interlocutor speaking in given timepoint.

First we build a pipeline with just one subject. Then we convert the code into a real pipeline of functions inside of the 'preprocessing.R' script.
```{r}
#load all files from one participant - Sub105
f_gest = 'raw_data/Gesture/Sub105RightHanded.csv'
f_dia = 'raw_data/Diarization/105.csv'
claps = read.csv("clean_data/all_claps.csv")
gesture = read.csv(f_gest)
turns = read.csv(f_dia)

#remove first row which is just NA
gesture = gesture[-1,]

#sampling of the gesture data is 100 Hz
sampling_gesture = 100

#get ID of the participant
ID = parse_number(f_gest)

#get row from claps data with info about the gap
gap = claps[claps$ID==ID,]

#inspect the confidence of the diarization system
sum((turns$Confidence<0.5)==T)
mean(turns$Confidence)
median(turns$Confidence)
sd(turns$Confidence)

#trim the gesture data to align it in time with the audio file - remove all rows before the first clap
  #calculate the datapoint/row where the clap happened
trim_gesture = gap$gestclap1 * sampling_gesture

#cutoff all rows below the trim_gesture value
gesture_aligned = gesture[trim_gesture:nrow(gesture),]

#trim the diarization data - throw away anything that happened before the first clap because there are no gesture data
trim_diarization = gap$audioclap1
turns_aligned = turns[turns$StartTime>trim_diarization,]

#create time variable in gesture_aligned in seconds+2decimals so that we don't have to convert time into sampling all the time
gesture_aligned$time =1:nrow(gesture_aligned)/100

#trim gesture data again - cut off the end based on last datapoint in diarization
trim_end = max(turns_aligned$EndTime)
gesture_final = gesture_aligned[gesture_aligned$time<=trim_end,]

#create interviewer variable - binary variable with 0=subject speaking 1=interviewer speaking and -1=nobody speaking
gesture_final$interviewer = -1

  #iterate over entries in turns_aligned and fill the interviewer variable with correct values
n=1
for (entry in 1:nrow(turns_aligned)) {
  row = turns_aligned[entry,]
  
  #utterance start
  start = row$StartTime
  
  #utterance end
  end = row$EndTime
  
  #who's speaking in this time
  speaker = row$Interlocutor
  
  #binarize the speaker => 1/0
  speaker = ifelse(speaker=='Interviewer', yes=1, no=0)
  
  #fill the interviewer variable
  
  #for some reason this approach skips values
  #utterance = seq(from=start, to=end, by=0.01)
  #gesture_final[gesture_final$time %in% s, 'interviewer'] = speaker
  
  gesture_final[(start*100):(end*100), 'interviewer'] = speaker
}


print(paste("Data from participant number", ID, "is clean.", sep = " "))

if (n%%5 == 0) {
  print(paste(n, "out of 86 processed."))
}

```

  

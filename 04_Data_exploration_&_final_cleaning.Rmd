---
title: "04 Data exploration and final cleaning"
author: "Dominik Klepl"
date: "11/8/2018"
output: html_document
---

In the folder "clean_data/ML_ready" there are csv files with data almost ready to be fed into some models.
The RQA was run both with one average set of parameters and with parameters optimized uniquely for every time-series. First it needs to be decided which of these two to use.
Some filtering and transformation of the data is needed first.

Here are all the ML ready data.
```{r}
library(lmerTest)
library(tidyverse)
library(ggthemes)
library(caret)
all = list.files("clean_data/ML_ready", full.names = T)
```

Load all data
```{r load data}
data_c1 = read.csv(all[1])
data_c2 = read.csv(all[2])
data_i1 = read.csv(all[3])
data_i2 = read.csv(all[3])
data_p1 = read.csv(all[5])
data_p2 = read.csv(all[6])
```

# Which RQA data should be used?
```{r which parameters}
data_c1 = subset(data_c1, nbr.null<1500)
data_c2 = subset(data_c2, nbr.null<1500)
data_i1 = subset(data_i1, nbr.null<500)
data_i2 = subset(data_i2, nbr.null<500)
data_p1 = subset(data_p1, nbr.null<500)
data_p2 = subset(data_p2, nbr.null<500)

data_c1 = na.omit(data_c1)
data_c2 = na.omit(data_c2)
data_i1 = na.omit(data_i1)
data_i2 = na.omit(data_i2)
data_p1 = na.omit(data_p1)
data_p2 = na.omit(data_p2)

c1 = glmer(diagnosis ~ RR + (1|ID), data_c1, family = "binomial", control = glmerControl(calc.derivs = F))
c2 = glmer(diagnosis ~ RR + (1|ID), data_c2, family = "binomial", control = glmerControl(calc.derivs = F))
i1 = glmer(diagnosis ~ RR + (1|ID), data_i1, family = "binomial", control = glmerControl(calc.derivs = F))
i2 = glmer(diagnosis ~ RR + (1|ID), data_i2, family = "binomial", control = glmerControl(calc.derivs = F))
p1 = glmer(diagnosis ~ RR + (1|ID), data_p1, family = "binomial", control = glmerControl(calc.derivs = F))
p2 = glmer(diagnosis ~ RR + (1|ID), data_p2, family = "binomial", control = glmerControl(calc.derivs = F))

summary(c1)
summary(c2)
summary(i1)
summary(i2)
summary(p1)
summary(p2)

#clean the environment
rm(list = ls()[-1])
```

None of the models shows a significant difference in RR across diagnosis. Therefore all models will use the RQA results obtained with one set of parameters.

Load only the data that will be used
```{r load data}
data_c = read.csv("clean_data/ML_ready/coordination_data_enhanced.csv")
data_i = read.csv("clean_data/ML_ready/interviewer_data_enhanced.csv")
data_p = read.csv("clean_data/ML_ready/participant_data_enhanced.csv")
```

save names of all extracted features
```{r}
features_c = colnames(data_c)
features_p = colnames(data_p)
  
write.csv(features_c, "Tables/Coordination_features.csv", row.names = F)      
write.csv(features_p, "Tables/Participant_features.csv", row.names = F) 
```

# Final data processing
Before making any machine learning, the data needs to be treated for missing values, corrupt data-splits (where the actigraphs were not picking up almost any signal) and remove duplicate feature columns and columns that don't include any ML usable information.

```{r clean up}
#the same should be done to all data files I have => write a function
prepare_data = function(data, single=T) {
  #remove NAs
  data = na.omit(data)
  
  #remove signals that is more than 90% comprised of zeroes
  if (single==F) {data$RR_diff=NULL}
  
  data = subset(data, RR>0)
  
  #rename columns
  colnames(data)[4]="time"
  data[,2] = ifelse(data[,2]==1, "Schizophrenic", "Control")
  data$RR=NULL

  return(data)
}

#now clean up all 3 datasets
data_c = prepare_data(data_c, single = F)
data_i = prepare_data(data_i)
data_p = prepare_data(data_p)
```

For the sake of plotting and inspecting the data we turn all features to correct data-classes. This will be required every time the data is loaded => write a function.
```{r change classes}
change_classes = function(data) {
  
  factor_cols = c("diagnosis", "right", "time")
  data[factor_cols] = lapply(data[factor_cols], as.factor)
  
  num_cols = c("n_utterances","nbr.null", "NRLINE", "maxL")
  data[num_cols] = lapply(data[num_cols], as.numeric)
  
  
  
  return(data)
}

data_c = change_classes(data_c)
data_i = change_classes(data_i)
data_p = change_classes(data_p)
```

## Class balance
The distribution is probably quite unbalanced, inspect the size of the imbalance and treat it appropriately.

Plot the sample sizes and save to a production-ready plot.
```{r plot class balance}
balance_c = ggplot(data_c, aes(x=diagnosis, fill=diagnosis))+
  geom_histogram(stat="count")+
  scale_x_discrete(labels=c("Control"="C", "Schizophrenic"="S"))+
  xlab("Coordination")+
  theme_few()+
  guides(fill=F)
balance_i = ggplot(data_i, aes(x=diagnosis, fill=diagnosis))+
  geom_histogram(stat="count")+
  scale_x_discrete(labels=c("Control"="C", "Schizophrenic"="S"))+
  theme_few()+
  guides(fill=F)+
  xlab("Interviewer")
balance_p = ggplot(data_p, aes(x=diagnosis, fill=diagnosis))+
  geom_histogram(stat="count")+
  scale_x_discrete(labels=c("Control"="C", "Schizophrenic"="S"))+
  theme_few()+
  guides(fill=F)+
  xlab("Participant")

library(grid)
class_balance = gridExtra::grid.arrange(balance_c, balance_i, balance_p, clip="on", ncol=3, top=textGrob("Sample size balance", gp=gpar(fontsize=20, font=3)))

ggsave("Figures/Class_balance.jpg", class_balance, width = 6, height = 2)
```

Get also the numbers for report.
```{r class balance numbers}
balance_data = data.frame(Patient = c(sum(data_c$diagnosis=="Schizophrenic"),
                                      sum(data_i$diagnosis=="Schizophrenic"),
                                      sum(data_p$diagnosis=="Schizophrenic")),
                          Control = c(sum(data_c$diagnosis=="Control"),
                                      sum(data_i$diagnosis=="Control"),
                                      sum(data_p$diagnosis=="Control")),
                          row.names = c("Coordination", 
                                        "Interviewer", 
                                        "Participant"))
write.csv(balance_data, "Tables/Class_balance.csv")
```

So I need to treat the imbalance in the data.
```{r}
data_c = downSample(data_c, data_c$diagnosis, yname="diagnosis")
data_i = downSample(data_i, data_i$diagnosis, yname="diagnosis")
data_p = downSample(data_p, data_p$diagnosis, yname="diagnosis")

data_c = data_c[,-63]
data_i = data_i[,-34]
data_p = data_p[,-34]
```


## Identify near-zero-variance features
Some features might be unusable for modelling. We use 2 criterions to identify such features: frequence ratio and percent of unique values.
```{r}
nzv_c = nearZeroVar(data_c[,-1:-3], saveMetrics = T)
nzv_i = nearZeroVar(data_i, saveMetrics = T)
nzv_p = nearZeroVar(data_p, saveMetrics = T)

plot_nzv = function(nzv, data) {
  library(ggrepel)
  nzv$Status = ifelse(nzv$zeroVar == T | nzv$nzv ==T, yes="Fail", no="Pass")
  nzv$Feature = rownames(nzv)
  
  plot = ggplot(nzv, aes(y=freqRatio, x=percentUnique, color=Status, label=Feature))+
  geom_point()+
  geom_label_repel(data=subset(nzv, Status=="Fail"), size=3, show.legend = F)+
  ggtitle(paste("Near Zero Variance Filter -", data))
  theme_few()
  
  return(plot)
}

nzv_c_plot = plot_nzv(nzv_c, "Coordination")
nzv_i_plot = plot_nzv(nzv_i, "Interviewer")
nzv_p_plot = plot_nzv(nzv_p, "Participant")

ggsave("Figures/nzv_filter_coordination.jpg", nzv_c_plot, width = 7, height = 2.5)
ggsave("Figures/nzv_filter_interviewer.jpg", nzv_i_plot, width = 7, height =2.5)
ggsave("Figures/nzv_filter_participant.jpg", nzv_p_plot, width = 7, height = 2.5)

remove_c = nearZeroVar(data_c, saveMetrics = F)
remove_i = nearZeroVar(data_i, saveMetrics = F)
remove_p = nearZeroVar(data_p, saveMetrics = F)

data_c = data_c[,-remove_c]
data_i = data_i[,-remove_i]
data_p = data_p[,-remove_p]
```

## Find potential linear combinations of features
I still need to consider this step. It's basically looking for interactions/correlations. And removing such features could hurt models that wouldn't be able to find the interaction themselves.
```{r}
#these colums might be considered for removal
linear_c = findLinearCombos(data_c[,-1:-4])
linear_i = findLinearCombos(data_i[,-1:-4])
linear_p = findLinearCombos(data_p[,-1:-4])

remove_c = (linear_c$remove)+4
remove_i = (linear_i$remove)+4
remove_p = (linear_p$remove)+4

colnames(data_c[,remove_c])
colnames(data_i[,remove_i])
colnames(data_p[,remove_p])

data_c = data_c[,-remove_c]
data_i = data_i[,-remove_i]
data_p = data_p[,-remove_p]

colnames(data_p)[remove_p]
colnames(data_c)[remove_c]
```

# Train-test split
```{r}
#vectors with IDs
ID_c = unique(data_c$ID)
ID_i = unique(data_i$ID)
ID_p = unique(data_p$ID)

sum((ID_c ==ID_i)==F)
sum((ID_c ==ID_p)==F)

#split the vectors into 2 while keeping samples from same ID together
trainIndex = createDataPartition(ID_c, p=0.75, list = F)
trainIndex = ID_c[trainIndex]

inTrain_c = which(data_c$ID %in% trainIndex)
inTrain_i = which(data_i$ID %in% trainIndex)
inTrain_p = which(data_p$ID %in% trainIndex)

data_c[-inTrain_c, "set"] = "test"
data_c[inTrain_c, "set"] = "train"

data_i[-inTrain_i, "set"] = "test"
data_i[inTrain_i, "set"] = "train"

data_p[-inTrain_p, "set"] = "test"
data_p[inTrain_p, "set"] = "train"
```

Now all 3 datasets are truely ML-ready. Therefore we can save the final version now.
```{r save data}
write.csv(data_c, "clean_data/ML_ready/Final/coordination_data_final.csv", row.names = F)
write.csv(data_i, "clean_data/ML_ready/Final/interviewer_data_final.csv", row.names = F)
write.csv(data_p, "clean_data/ML_ready/Final/participant_data_final.csv", row.names = F)
```

# Get stats about n_samples and IDs in train-test data and class balance
```{r}
data_c = read.csv("clean_data/ML_ready/Final/coordination_data_final.csv")
data_c_train = subset(data_c, set=="train")
data_c_test = subset(data_c, set=="test")

data_p = read.csv("clean_data/ML_ready/Final/participant_data_final.csv")
data_p_train = subset(data_p, set=="train")
data_p_test = subset(data_p, set=="test")
library(tidyverse)

samples_info = function(data) {
  #number of samples per participant
  n_samples = data %>% group_by(ID) %>% summarise(samples=n(), diagnosis=diagnosis[1])
  
  samples_dia_effect = lm(samples ~ diagnosis, n_samples)
  result = summary(samples_dia_effect)$coefficients[2,1:4]
  
  info = data.frame(
    #number of participants
    size = nrow(data),
    ID = length(unique(data$ID)),
    #class balance
    Schizophrenic = sum(data$diagnosis=="Schizophrenic"),
    Control = sum(data$diagnosis=="Control"),
    `samples mean` = mean(n_samples$samples),
    `samples SD` = sd(n_samples$samples),
    ß = result[1],
    `t value` = result[3],
    `Pr(>|t|)` = result[4] 
  )
  
  return(info)
}

info_train_c = samples_info(data_c_train)
info_test_c = samples_info(data_c_test)
info_c = rbind(info_train_c, info_test_c)
row.names(info_c)=c("Training", "Validation")

info_train_p = samples_info(data_p_train)
info_test_p = samples_info(data_p_test)
info_p = rbind(info_train_p, info_test_p)
row.names(info_p)=c("Training", "Validation")

info = rbind(info_p, info_c)

write.csv(info, "Tables/Balance_sample_size_all.csv")
```


